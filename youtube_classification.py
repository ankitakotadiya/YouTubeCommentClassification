# -*- coding: utf-8 -*-
"""Youtube_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AudTbXET8HywadKI5Nwe-nP2TPF7TPDp

# YouTube Comment Classification ('Normal' or 'Toxic')
First I will install necessary libraries to load data into environment
"""

# !pip install transformers
# !pip install emoji
# !pip install langdetect

#Numeric, DataFrame, and Visulization
import numpy as np
import pandas as pd
import matplotlib

#Text Processing
import glob
import string
import re

#Emoji to text conversion
import emoji

#To show percentage of process data
from tqdm.auto import tqdm

#Tensorflow and BERT
import tensorflow as tf
from transformers import BertTokenizer
from transformers import BertTokenizer,BertConfig, BertForSequenceClassification, AdamW

#Sentiment Score
from textblob import TextBlob

#Train Test Split
from sklearn.model_selection import train_test_split

#Detect Language
from langdetect import detect

import os


"""# Data Loading
### I have collected dataset from kaggle .csv extension and using pandas load into environment
"""

filepath = 'UScomments.csv' #File stored in My Drive
current_dir = os.getcwd()
full_filepath = os.path.join(current_dir, filepath)
df = pd.read_csv(full_filepath) #Read CSV

"""### Highlight of the Data"""

df.head()

"""### Shape of the Dataset"""

df.shape

"""# Data Preprocessing
### The dataset I have received is unlabelled in order to train classification model I require labelled data so using 'TextBlob' library I have generated sentiment score of the data and created labelled data.
"""

from textblob.en import polarity
pol=[] # list which will contain the polarity of the comments
for i in df.comment_text.values:
    try:
        analysis =TextBlob(i)
        pol.append(analysis.sentiment.polarity)

    except:
        pol.append(0)

df['pol'] = pol #Created new column and assigned polarity as values
df['pol'] = df['pol'].astype(float) #Converted string to float type, ease of calculations

"""### The polarity is -1 for negative, +1 for positive and 0 for neutral. However I need only normal and Toxic and dataset is quite huge, my system's configuration would to process that large dataset so I took slice from the main dataset and ruffly 2094 samples I have collected."""

toxic = df[(df.pol <= -0.8)]
neutral = df[(df.pol == 0.0)][:500]
positive = df[(df.pol >= 1.0)][:500]

"""### DataFrame Concetenation"""

df_main = pd.concat([toxic, neutral, positive], ignore_index=False)
df_main

def map_pol_value(value): #This function will convert polarity to labels 0 and 1
    if value <= -0.8:
        return 1
    elif value >= 0.0:
        return 0

df_main.pol = df_main.pol.apply(map_pol_value)
df_main #Here 0 is Normal and 1 is Toxic

df_main.describe #Description about data

df_main.isna().sum() #Retun sum of NaN values if not then it will return 0

def is_english(text): #This functions detects if any language other than english
    try:
        lang = detect(text)
        return lang == 'en'
    except:
        return False

def preprocess_text(input_text):
    # Remove HTML tags and extra spaces
    cleaned_text = re.sub(r'<.*?>', '', input_text)
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text)

    # Remove URLs
    cleaned_text = re.sub(r'http\S+', '', cleaned_text)

    # Remove digits
    cleaned_text = re.sub(r'\d', '', cleaned_text)

    # Convert emojis to text and remove duplicates
    def replace_emoji(match):
        emoji_text = emoji.demojize(match.group())
        return emoji_text

    # Replace emojis with their text representation
    cleaned_text = emoji.demojize(cleaned_text)

    # Remove duplicate emojis
    emojis = re.findall(r'(:[^:]+:)', cleaned_text)
    cleaned_emojis = list(set(emojis))  # Convert to set and back to list to remove duplicates
    emoji_pattern = '|'.join(re.escape(e) for e in cleaned_emojis)
    emoji_regex = re.compile(emoji_pattern)
    cleaned_text = emoji_regex.sub(replace_emoji, cleaned_text)

    # Add space between different emoji texts
    cleaned_text = re.sub(r'(:\w+:)(:\w+:)', r'\1 \2', cleaned_text)

    # Remove punctuation (except underscore) and unnecessary spaces between words
    punctuations_to_remove = ''.join([c for c in string.punctuation if c != '_'])
    cleaned_text = re.sub(f"[{re.escape(punctuations_to_remove)}]", '', cleaned_text)
    cleaned_text = ' '.join(cleaned_text.split())

    # Convert to lowercase and strip
    cleaned_text = cleaned_text.lower().strip()

    return cleaned_text

"""# Feature Selection
In my dataset I have two unnecesarry colums likes and replies I will drop it as don't require to train model.
"""

df_main = df_main.drop(columns = ['likes','replies'])
df_main.columns = ['video_id','comment_text','labels'] #Rename header

"""# Train Validation Splits
I have chosen 90% of the data as training and 10% evaluate model's performance.
"""

X_train, X_test = train_test_split(df_main, stratify=df_main['labels'],test_size=0.1)

len(X_train) #Length of the training set.

"""### I will apply data processing only on training set to avoid data leakage issue."""

X_train['comment_text'] = X_train["comment_text"].apply(lambda x: preprocess_text(x))

X_train.head()#Hightlight of processed Data.

"""# Feature Extraction"""

# Create a tokenizer instance using the 'bert-base-cased' pre-trained model
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')

# Create two NumPy arrays to store input IDs and attention masks

# X_input_ids will store the input IDs for each sample
X_input_ids = np.zeros((len(X_train), 256))

# X_attn_masks will store the attention masks for each sample
X_attn_masks = np.zeros((len(X_train), 256))

def generate_training_data(df_data, ids, masks, tokenizer):
    # Loop through each comment in the DataFrame using tqdm for progress tracking
    for i, comment in tqdm(enumerate(df_data['comment_text'])):
        # Tokenize the comment using the provided tokenizer
        tokenized_text = tokenizer.encode_plus(
            comment,
            max_length=256,               # Maximum sequence length
            truncation=True,              # Truncate if the comment exceeds max length
            padding='max_length',         # Pad the sequence to the max length
            add_special_tokens=True,      # Add [CLS] and [SEP] tokens
            return_tensors='tf'           # Return TensorFlow tensors
        )

        # Store the token IDs in the 'ids' array for the i-th sample
        ids[i, :] = tokenized_text.input_ids

        # Store the attention masks in the 'masks' array for the i-th sample
        masks[i, :] = tokenized_text.attention_mask

    # Return the populated 'ids' and 'masks' arrays
    return ids, masks

# Generate training data using the generate_training_data function
X_input_ids, X_attn_masks = generate_training_data(X_train, X_input_ids, X_attn_masks, tokenizer)

len(X_input_ids)

# Extract the 'labels' column from the training data (X_train)
labels = X_train['labels']

len(labels)

# Create a TensorFlow Dataset from the input IDs, attention masks, and labels.
dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))

# Take the first example from the dataset and print it.
dataset.take(1)

# Define a mapping function for the dataset elements.
# This function takes input IDs, attention masks, and labels as arguments,
# and returns a dictionary containing the input features and labels.
def QADatasetMapFunction(input_ids, attn_masks, labels):
    return {
        'input_ids': input_ids,
        'attention_mask': attn_masks
    }, labels

dataset = dataset.map(QADatasetMapFunction)#Mapping dataset

dataset = dataset.shuffle(10000).batch(32, drop_remainder=True) # batch size, drop any left out tensor

# Import the TFBertModel class from the transformers library.
# This class represents the BERT model architecture.
from transformers import TFBertModel

# Load the pre-trained BERT base model using its name 'bert-base-cased'.
# This will initialize the BERT model with pre-trained weights.
model = TFBertModel.from_pretrained('bert-base-cased') # bert base model with pretrained weights

# Create input layers for input_ids and attention_mask.
# These layers will hold the tokenized input and attention masks respectively.
input_ids = tf.keras.layers.Input(shape=(256,), name='input_ids', dtype='int32')
attn_masks = tf.keras.layers.Input(shape=(256,), name='attention_mask', dtype='int32')

# Pass the input_ids and attention_mask through the BERT model.
# The [1] index refers to the pooled output layer of the BERT model.
bert_embds = model.bert(input_ids, attention_mask=attn_masks)[1]

# Add a dense intermediate layer for feature transformation.
# This layer has 512 units and uses ReLU activation.
intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name='intermediate_layer')(bert_embds)

# Add the output layer with sigmoid activation.
# This layer produces a single output value representing the toxicity probability.
output_layer = tf.keras.layers.Dense(1, activation='sigmoid', name='output_layer')(intermediate_layer)

# Create the classification model using the input and output layers.
# This model takes input_ids and attention_mask as input and produces the output from the output layer.
cls_model = tf.keras.Model(inputs=[input_ids, attn_masks], outputs=output_layer)

# Print a summary of the model's architecture.
cls_model.summary()

# Define the optimizer for model training.
# Adam optimizer with a learning rate of 1e-5 is used.
optim = tf.keras.optimizers.Adam(learning_rate=1e-5)

# Define the loss function for model training.
# BinaryCrossentropy loss is commonly used for binary classification tasks.
loss_func = tf.keras.losses.BinaryCrossentropy()

# Define the accuracy metric to monitor during training.
# CategoricalAccuracy computes the accuracy between predicted and true labels.
acc = tf.keras.metrics.CategoricalAccuracy('accuracy')

# Compile the model using the specified optimizer, loss function, and metrics.
cls_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])

"""# Model Training"""

# Train the model using the training dataset.
hist = cls_model.fit(
    dataset, epochs=1
)

"""# Model Evaluation"""

#Create input ids and attention mask for test dataset
X_unlabeled_input_ids = np.zeros((len(X_test), 256))
X_unlabeled_attn_masks = np.zeros((len(X_test), 256))

#Generate test training data
X_unlabeled_input_ids, X_unlabeled_attn_masks = generate_training_data(X_test, X_unlabeled_input_ids, X_unlabeled_attn_masks, tokenizer)

# Evaluate the model on unlabeled test data.
score = cls_model.evaluate({'input_ids': X_unlabeled_input_ids, 'attention_mask': X_unlabeled_attn_masks},X_test.labels)

score

"""### Save Model"""

# Convert the trained model to JSON format and save it to a file.
# Convert the model to JSON format
model_json = cls_model.to_json()

# Save the JSON representation to a file
with open("YouTubeClassification_json.json", "w") as json_file:
    json_file.write(model_json)

cls_model.save_weights("YouTubeClassification_weights.h5")#Save models weight

"""### Load Model




"""

from keras.models import model_from_json
# Load the model architecture from the JSON file
with open("YouTubeClassification_json.json", "r") as json_file:
    loaded_model_json = json_file.read()

# Create a new model from the loaded JSON architecture
loaded_model = model_from_json(loaded_model_json)

# Load the model weights from a file (e.g., a saved HDF5 file)
loaded_model.load_weights("YouTubeClassification_weights.h5")

# Compile the loaded model
loaded_model.compile(optimizer=optim, loss=loss_func, metrics=[acc])

"""# Scrap Test Data From YouTube"""

import requests
from urllib.parse import urlparse, parse_qs
import csv

# Your YouTube API key
API_KEY = "AIzaSyCXUkOuHoqdKUMozozDpOxywpOX8Wsq9zQ"

# List of YouTube video URLs
youtube_urls = ['https://www.youtube.com/watch?v=MF_bkz3v3mE',
                'https://www.youtube.com/watch?v=JMUxmLyrhSk&t=1s',
                'https://www.youtube.com/watch?v=dTRBnHtHehQ',
                'https://www.youtube.com/watch?v=HU7TMhkQHhA',
                'https://www.youtube.com/watch?v=zi7qIMXNhA0',
                'https://www.youtube.com/watch?v=AYXfaVD5o40',
                'https://www.youtube.com/watch?v=VxMHHqSOSTk',
                'https://www.youtube.com/watch?v=zd4ALKv8Das',
                'https://www.youtube.com/watch?v=EW4dEzfBst0',
                'https://www.youtube.com/watch?v=jGwO_UgTS7I&list=PLoROMvodv4rMiGQp3WXShtMGgzqpfVfbU']

# Initialize an empty list to store video IDs
video_ids = []

# Loop through each YouTube URL
for url in youtube_urls:
    # Parse the URL to get its components
    url_parts = urlparse(url)

    # Extract query parameters from the URL
    query_params = parse_qs(url_parts.query)

    # Extract the 'v' parameter, which corresponds to the video ID
    video_id = query_params["v"][0]

    # Append the video ID to the list
    video_ids.append(video_id)

comments_dic = {'video_id':[],'comment_text':[]}
# Iterate through each video and fetch comments
for video_id in video_ids:
    # Fetch comments for the video
    comments_url = f"https://www.googleapis.com/youtube/v3/commentThreads?key={API_KEY}&videoId={video_id}&part=snippet&maxResults=100"
    comments_response = requests.get(comments_url)
    comments_data = comments_response.json()

    # Extract and print comments
    for comment_item in comments_data["items"]:
        # print(comment_item)
        comment = comment_item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
        # print(comment)
        comments_dic["video_id"].append(video_id)
        comments_dic['comment_text'].append(comment)

test_df = pd.DataFrame(comments_dic) #Create DataFrame from comment dictionary.

if len(test_df) > 500:
  test_df = test_df[:500]

test_df

test_df['comment_text'] = test_df["comment_text"].apply(lambda x: preprocess_text(x)) #Data Preprocessing

X_unlabeled_input_ids = np.zeros((len(test_df), 256))
X_unlabeled_attn_masks = np.zeros((len(test_df), 256))

X_unlabeled_input_ids, X_unlabeled_attn_masks = generate_training_data(test_df, X_unlabeled_input_ids, X_unlabeled_attn_masks, tokenizer)

predicted_labels = cls_model.predict({'input_ids': X_unlabeled_input_ids, 'attention_mask': X_unlabeled_attn_masks})

# predicted_labels = np.argmax(predicted_labels, axis=1)
# Set threshold
threshold = 0.5

# Convert to binary values (0 or 1)
binary_array = np.where(predicted_labels > threshold, 1, 0).flatten()
binary_array

#Create label column and assign predicted labels
test_df['labels'] = binary_array

label_mapping = {
    0: "Normal",
    1: "Toxic"
}

#Map 0 as Normal and 1 as Toxic
test_df.labels = test_df.labels.map(label_mapping)

test_df

"""# Store Preprocessed data into PostgreSQL"""

# Install PostgreSQL adapter
# !pip install psycopg2

# import psycopg2
# from sqlalchemy import create_engine

# # Replace these placeholders with your PostgreSQL database credentials
# db_params = {
#     'dbname': 'YT_Classification',
#     'user': 'ankitak',
#     'password': 'Test@123',
#     'host': 'your_host',
#     'port': 'your_port'
# }


# # Create a connection to the PostgreSQL database
# engine = create_engine(
#     f'postgresql+psycopg2://{db_params["user"]}:{db_params["password"]}@{db_params["host"]}:{db_params["port"]}/{db_params["dbname"]}'
# )

# # Replace 'your_table_name' with the name of your table
# table_name = 'Comments'
# test_df.to_sql(table_name, engine, if_exists='replace', index=False)